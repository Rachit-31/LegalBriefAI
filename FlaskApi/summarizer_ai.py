# -*- coding: utf-8 -*-
"""Summarizer_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eya1NCexHQE8zfcPleBb7L6-tLGyJwCR
"""

pip install transformers

# for installing the transformer library
pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q

"""For checking the gpu"""

nvidia-smi

"""Installing desired libraries and dependencies"""

from tqdm import tqdm

from transformers import pipeline

import matplotlib.pyplot as plt
from datasets import load_dataset
import pandas as pd
from datasets import load_dataset, load_metric

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

import nltk
from nltk.tokenize import sent_tokenize


import torch

nltk.download("punkt")

"""checking if i am runnign on cpu or gpu"""

# here we  have initialise the device now model will work on this
device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Here we called the model from hugging face in " " modle name is written
# for second line we are initialising the tokeniser for our model
model_ckpt = "google/pegasus-cnn_dailymail"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

"""Initialise pegasus model"""

# for downloading the model
model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)

"""Load the data"""

dataset_samsum=load_dataset("samsum")
dataset_samsum

# summarising the dialog to check the working
# it will pirnt the summary that is written in the dataset
print("\nDialogue:")
print(dataset_samsum["test"][1]["dialogue"])

print("\nSummary")
print(dataset_samsum["test"][1]["summary"])

"""Evaluating the peagsus on samsum"""

dialogue=dataset_samsum['test'][0]['dialogue']
dialogue

pipe=pipeline('summarization',model=model_ckpt)

pipe = pipeline('summarization', model = model_ckpt )
pipe_out=pipe(dialogue)
pipe_out

# for above summarized text <n> we are getting so to remove this we are writting another code
print(pipe_out[0]['summary_text'].replace(".<n>", ".\n"))


# this will print the summary of cell 11 here we go

"""Checking the accuracy of the model
NOTE: WE CANNOT FIND THE ACCURACY OF THIS MODLE BECAUSE WE NEED GPU WHICH IS NOT SUPPORTED BY OUR COMPUTER
"""

# DO NOT MEMORISE THE FUNCTION for nlp
def generate_batch_sized_chunks(list_of_elements, batch_size):
    """split the dataset into smaller batches that we can process simultaneously
    Yield successive batch-sized chunks from list_of_elements."""
    for i in range(0, len(list_of_elements), batch_size):
        yield list_of_elements[i : i + batch_size]


def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,
                               batch_size=16, device=device,
                               column_text="article",
                               column_summary="highlights"):
    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))
    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))

    for article_batch, target_batch in tqdm(
        zip(article_batches, target_batches), total=len(article_batches)):

        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,
                        padding="max_length", return_tensors="pt")

        summaries = model.generate(input_ids=inputs["input_ids"].to(device),
                         attention_mask=inputs["attention_mask"].to(device),
                         length_penalty=0.8, num_beams=8, max_length=128)
        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''

        # Finally, we decode the generated texts,
        # replace the  token, and add the decoded texts with the references to the metric.
        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,
                                clean_up_tokenization_spaces=True)
               for s in summaries]

        decoded_summaries = [d.replace("", " ") for d in decoded_summaries]


        metric.add_batch(predictions=decoded_summaries, references=target_batch)

    #  Finally compute and return the ROUGE scores.
    score = metric.compute()
    return score

# # cheching the accuracy we are using row matrix
# # NOTE THAT WE ARE CHECKING ACCURACY WITH PRETRAINED MODEL NOT TRAINED MODEL AFTER FINE TUNNING
# rouge_metric = load_metric('rouge')
# score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary='summary', batch_size=8)

# rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
# rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )

# pd.DataFrame(rouge_dict, index = ['pegasus'])

"""Converting into numerical representation"""

def convert_examples_to_features(example_batch):
    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )

    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )

    return {
        'input_ids' : input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'labels': target_encodings['input_ids']
    }

dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)

dialogue_token_len = len([tokenizer.encode(s) for s in dataset_samsum['train']['dialogue']])

def convert_examples_to_features(example_batch):
    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )

    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )

    return {
        'input_ids' : input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'labels': target_encodings['input_ids']
    }

dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)

from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)

"""setting the training arguments"""

!pip install accelerate>=0.21.0
!pip install accelerate -U
!pip install transformers[torch]

from google.colab import drive
drive.mount('/content/drive')

!pip show transformers accelerate

!pip install transformers==4.30

from transformers import TrainingArguments, Trainer

trainer_args = TrainingArguments(
    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,
    per_device_train_batch_size=1, per_device_eval_batch_size=1,
    weight_decay=0.01, logging_steps=10,
    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,
    gradient_accumulation_steps=16
)

trainer = Trainer(
    model=model_pegasus,
    args=trainer_args,
    tokenizer=tokenizer,
    data_collator=seq2seq_data_collator,
    train_dataset=dataset_samsum_pt["train"],
    eval_dataset=dataset_samsum_pt["validation"]
)

trainer.train()

## Save model
model_pegasus.save_pretrained("Text_Summarizer")

## Save tokenizer
tokenizer.save_pretrained("tokenizer")

from google.colab import drive
drive.mount('/drive/content')

from google.colab import drive
drive.mount('/content/drive')